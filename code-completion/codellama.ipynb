{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, Conversation\n",
    "\n",
    "chatbot = pipeline(model=\"microsoft/DialoGPT-medium\")\n",
    "conversation = Conversation(\"Going to the movies tonight - any suggestions?\")\n",
    "conversation = chatbot(conversation)\n",
    "conversation.generated_responses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_masker = pipeline(model=\"bert-base-uncased\")\n",
    "tokenizer_kwargs = {\"truncation\": True}\n",
    "fill_masker(\n",
    "    \"This is a simple [MASK]. \"\n",
    "    + \"...with a large amount of repeated text appended. \" * 100,\n",
    "    tokenizer_kwargs=tokenizer_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.add_user_input(\"Is it an action movie?\")\n",
    "conversation = chatbot(conversation)\n",
    "conversation.generated_responses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(model=\"gpt2\")\n",
    "print(generator(\"I can't believe you did such a \", do_sample=False))\n",
    "\n",
    "# These parameters will return suggestions, and only the newly created text making it easier for prompting suggestions.\n",
    "print(generator(\"My tart needs some\", num_return_sequences=4, return_full_text=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "pipe = pipeline(\"text-classification\")\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __len__(self):\n",
    "        return 5000\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return \"This is a test\"\n",
    "\n",
    "\n",
    "dataset = MyDataset()\n",
    "\n",
    "for batch_size in [256]:\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Streaming batch_size={batch_size}\")\n",
    "    for out in tqdm(pipe(dataset, batch_size=batch_size), total=len(dataset)):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers.git@mainNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Cloning https://github.com/huggingface/transformers.git (to revision main) to c:\\users\\shafi\\appdata\\local\\temp\\pip-req-build-78zff4br\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit ced9fd86f55ebb6b656c273f6e23f8ba50652f83\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: accelerate in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (0.24.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from transformers==4.36.0.dev0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from transformers==4.36.0.dev0) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from transformers==4.36.0.dev0) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from transformers==4.36.0.dev0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from transformers==4.36.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from transformers==4.36.0.dev0) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from transformers==4.36.0.dev0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from transformers==4.36.0.dev0) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from transformers==4.36.0.dev0) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from transformers==4.36.0.dev0) (4.66.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from accelerate) (2.1.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.36.0.dev0) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.36.0.dev0) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers==4.36.0.dev0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from requests->transformers==4.36.0.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from requests->transformers==4.36.0.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from requests->transformers==4.36.0.dev0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from requests->transformers==4.36.0.dev0) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git 'C:\\Users\\shafi\\AppData\\Local\\Temp\\pip-req-build-78zff4br'\n"
     ]
    }
   ],
   "source": [
    "%pip install git+https://github.com/huggingface/transformers.git@main accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50f0c6397b3940f3ba214b03e7ff372d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load model TheBloke/CodeLlama-7B-GGUF with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\shafi\\source\\Notebooks\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\shafi\\source\\Notebooks\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 566, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\shafi\\source\\Notebooks\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 3098, in from_pretrained\n    raise EnvironmentError(\nOSError: TheBloke/CodeLlama-7B-GGUF does not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\shafi\\source\\Notebooks\\code-completion\\codellama.ipynb Cell 9\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shafi/source/Notebooks/code-completion/codellama.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shafi/source/Notebooks/code-completion/codellama.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Use a pipeline as a high-level helper\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/shafi/source/Notebooks/code-completion/codellama.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m pipeline \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39;49mpipeline(\u001b[39m\"\u001b[39;49m\u001b[39mtext-generation\u001b[39;49m\u001b[39m\"\u001b[39;49m, model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mTheBloke/CodeLlama-7B-GGUF\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shafi/source/Notebooks/code-completion/codellama.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# pipeline = transformers.pipeline(\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shafi/source/Notebooks/code-completion/codellama.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m#     \"text-generation\",\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shafi/source/Notebooks/code-completion/codellama.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m#     model=model,\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shafi/source/Notebooks/code-completion/codellama.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# )\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shafi/source/Notebooks/code-completion/codellama.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m sequences \u001b[39m=\u001b[39m pipeline(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shafi/source/Notebooks/code-completion/codellama.ipynb#X16sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mimport socket\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mdef ping_exponential_backoff(host: str):\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shafi/source/Notebooks/code-completion/codellama.ipynb#X16sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     do_sample\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shafi/source/Notebooks/code-completion/codellama.ipynb#X16sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     max_length\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shafi/source/Notebooks/code-completion/codellama.ipynb#X16sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\shafi\\source\\Notebooks\\.venv\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:870\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m framework \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    869\u001b[0m     model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[1;32m--> 870\u001b[0m     framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    871\u001b[0m         model,\n\u001b[0;32m    872\u001b[0m         model_classes\u001b[39m=\u001b[39;49mmodel_classes,\n\u001b[0;32m    873\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m    874\u001b[0m         framework\u001b[39m=\u001b[39;49mframework,\n\u001b[0;32m    875\u001b[0m         task\u001b[39m=\u001b[39;49mtask,\n\u001b[0;32m    876\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs,\n\u001b[0;32m    877\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[0;32m    878\u001b[0m     )\n\u001b[0;32m    880\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n\u001b[0;32m    881\u001b[0m hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mc:\\Users\\shafi\\source\\Notebooks\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:282\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[39mfor\u001b[39;00m class_name, trace \u001b[39min\u001b[39;00m all_traceback\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    281\u001b[0m             error \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwhile loading with \u001b[39m\u001b[39m{\u001b[39;00mclass_name\u001b[39m}\u001b[39;00m\u001b[39m, an error is thrown:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mtrace\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 282\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    283\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not load model \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m}\u001b[39;00m\u001b[39m with any of the following classes: \u001b[39m\u001b[39m{\u001b[39;00mclass_tuple\u001b[39m}\u001b[39;00m\u001b[39m. See the original errors:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00merror\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m         )\n\u001b[0;32m    286\u001b[0m \u001b[39mif\u001b[39;00m framework \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    287\u001b[0m     framework \u001b[39m=\u001b[39m infer_framework(model\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Could not load model TheBloke/CodeLlama-7B-GGUF with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"c:\\Users\\shafi\\source\\Notebooks\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 269, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\shafi\\source\\Notebooks\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 566, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\shafi\\source\\Notebooks\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 3098, in from_pretrained\n    raise EnvironmentError(\nOSError: TheBloke/CodeLlama-7B-GGUF does not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.\n\n\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"codellama/CodeLlama-7b-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "# Use a pipeline as a high-level helper\n",
    "pipeline = transformers.pipeline(\"text-generation\", model=\"TheBloke/CodeLlama-7B-GGUF\")\n",
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "# )\n",
    "\n",
    "sequences = pipeline(\n",
    "    \"import socket\\n\\ndef ping_exponential_backoff(host: str):\",\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    temperature=0.1,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Base ctransformers with no GPU acceleration\n",
    "%pip install ctransformers>=0.2.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96caf53df1314ccebc4c94d360c81762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1850490a56094a28a3f26bf65ad45bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "print(\"Enter a number\")\n",
      "userInput = input()\n",
      "\n",
      "try:\n",
      "  userNumber = int(userInput)\n",
      " "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TheBloke/CodeLlama-7B-GGUF\",\n",
    "    model_file=\"codellama-7b.q4_K_M.gguf\",\n",
    "    model_type=\"llama\",\n",
    "    gpu_layers=0,\n",
    ")\n",
    "\n",
    "for text in llm(\n",
    "    \"\"\"def fibonacci(n):\n",
    "  if n == 0:\n",
    "    return 0\n",
    "  elif n == 1:\n",
    "    return 1\n",
    "  else:\n",
    "    return fibonacci(n - 2) + fibonacci(n - 1)\"\"\",\n",
    "    max_new_tokens=30,\n",
    "    stream=True,\n",
    "):\n",
    "    print(text, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
