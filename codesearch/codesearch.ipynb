{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: openai in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (0.28.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from pandas) (1.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from openai) (3.8.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from matplotlib) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from matplotlib) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from requests>=2.20->openai) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from requests>=2.20->openai) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from requests>=2.20->openai) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from aiohttp->openai) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from tqdm->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas openai matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DEF_PREFIXES = ['def ', 'async def ']\n",
    "NEWLINE = '\\n'\n",
    "\n",
    "def get_function_name(code):\n",
    "    \"\"\"\n",
    "    Extract function name from a line beginning with 'def' or 'async def'.\n",
    "    \"\"\"\n",
    "    for prefix in DEF_PREFIXES:\n",
    "        if code.startswith(prefix):\n",
    "            return code[len(prefix): code.index('(')]\n",
    "\n",
    "def get_until_no_space(all_lines, i):\n",
    "    \"\"\"\n",
    "    Get all lines until a line outside the function definition is found.\n",
    "    \"\"\"\n",
    "    ret = [all_lines[i]]\n",
    "    for j in range(i + 1, len(all_lines)):\n",
    "        if len(all_lines[j]) == 0 or all_lines[j][0] in [' ', '\\t', ')']:\n",
    "            ret.append(all_lines[j])\n",
    "        else:\n",
    "            break\n",
    "    return NEWLINE.join(ret)\n",
    "\n",
    "def get_functions(filepath):\n",
    "    \"\"\"\n",
    "    Get all functions in a Python file.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding = 'utf8') as file:\n",
    "        all_lines = file.read().replace('\\r', NEWLINE).split(NEWLINE)\n",
    "        for i, l in enumerate(all_lines):\n",
    "            for prefix in DEF_PREFIXES:\n",
    "                if l.startswith(prefix):\n",
    "                    code = get_until_no_space(all_lines, i)\n",
    "                    function_name = get_function_name(code)\n",
    "                    yield {\n",
    "                        'code': code,\n",
    "                        'function_name': function_name,\n",
    "                        'filepath': filepath,\n",
    "                    }\n",
    "                    break\n",
    "\n",
    "def extract_functions_from_repo(code_root):\n",
    "    \"\"\"\n",
    "    Extract all .py functions from the repository.\n",
    "    \"\"\"\n",
    "    code_files = list(code_root.glob('**/*.py'))\n",
    "\n",
    "    num_files = len(code_files)\n",
    "    print(f'Total number of .py files: {num_files}')\n",
    "\n",
    "    if num_files == 0:\n",
    "        print('Verify openai-python repo exists and code_root is set correctly.')\n",
    "        return None\n",
    "\n",
    "    all_funcs = [\n",
    "        func\n",
    "        for code_file in code_files\n",
    "        for func in get_functions(str(code_file))\n",
    "    ]\n",
    "\n",
    "    num_funcs = len(all_funcs)\n",
    "    print(f'Total number of functions extracted: {num_funcs}')\n",
    "\n",
    "    return all_funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shafi\n",
      "C:\\Users\\shafi\\source\\repos\\llama-cpp-python\n",
      "Total number of .py files: 50\n",
      "Total number of functions extracted: 218\n"
     ]
    }
   ],
   "source": [
    "# Set user root directory to the 'openai-python' repository\n",
    "root_dir = Path.home()\n",
    "print(root_dir)\n",
    "\n",
    "# Assumes the 'openai-python' repository exists in the user's root directory\n",
    "code_root = root_dir / 'source/repos/llama-cpp-python'\n",
    "print(code_root)\n",
    "\n",
    "# Extract all functions from the repository\n",
    "all_funcs = extract_functions_from_repo(code_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (5.17.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: scipy in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (1.11.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from plotly) (8.2.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from plotly) (23.2)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from scipy) (1.26.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install plotly scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (1.3.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from scikit-learn) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\shafi\\source\\notebooks\\.venv\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>function_name</th>\n",
       "      <th>filepath</th>\n",
       "      <th>code_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>def make_request(url, params=None):\\n    print...</td>\n",
       "      <td>make_request</td>\n",
       "      <td>docker\\open_llama\\hug_model.py</td>\n",
       "      <td>[-0.0029997548554092646, 0.006617534905672073,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>def check_magic_and_version(filename):\\n    wi...</td>\n",
       "      <td>check_magic_and_version</td>\n",
       "      <td>docker\\open_llama\\hug_model.py</td>\n",
       "      <td>[0.0019175842171534896, -0.011885509826242924,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>def download_file(url, destination):\\n    prin...</td>\n",
       "      <td>download_file</td>\n",
       "      <td>docker\\open_llama\\hug_model.py</td>\n",
       "      <td>[-0.013736424967646599, 0.0036590476520359516,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>def get_user_choice(model_list):\\n    # Print ...</td>\n",
       "      <td>get_user_choice</td>\n",
       "      <td>docker\\open_llama\\hug_model.py</td>\n",
       "      <td>[-0.009958215989172459, -0.01502623688429594, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>def main():\\n    # Create an argument parser\\n...</td>\n",
       "      <td>main</td>\n",
       "      <td>docker\\open_llama\\hug_model.py</td>\n",
       "      <td>[-0.024131689220666885, 0.026415444910526276, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code            function_name  \\\n",
       "0  def make_request(url, params=None):\\n    print...             make_request   \n",
       "1  def check_magic_and_version(filename):\\n    wi...  check_magic_and_version   \n",
       "2  def download_file(url, destination):\\n    prin...            download_file   \n",
       "3  def get_user_choice(model_list):\\n    # Print ...          get_user_choice   \n",
       "4  def main():\\n    # Create an argument parser\\n...                     main   \n",
       "\n",
       "                         filepath  \\\n",
       "0  docker\\open_llama\\hug_model.py   \n",
       "1  docker\\open_llama\\hug_model.py   \n",
       "2  docker\\open_llama\\hug_model.py   \n",
       "3  docker\\open_llama\\hug_model.py   \n",
       "4  docker\\open_llama\\hug_model.py   \n",
       "\n",
       "                                      code_embedding  \n",
       "0  [-0.0029997548554092646, 0.006617534905672073,...  \n",
       "1  [0.0019175842171534896, -0.011885509826242924,...  \n",
       "2  [-0.013736424967646599, 0.0036590476520359516,...  \n",
       "3  [-0.009958215989172459, -0.01502623688429594, ...  \n",
       "4  [-0.024131689220666885, 0.026415444910526276, ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai.embeddings_utils import get_embedding\n",
    "\n",
    "df = pd.DataFrame(all_funcs)\n",
    "df['code_embedding'] = df['code'].apply(lambda x: get_embedding(x, engine='text-embedding-ada-002'))\n",
    "df['filepath'] = df['filepath'].map(lambda x: Path(x).relative_to(code_root))\n",
    "df.to_csv(\"data/code_search_openai-python.csv\", index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.embeddings_utils import cosine_similarity\n",
    "\n",
    "def search_functions(df, code_query, n=3, pprint=True, n_lines=7):\n",
    "    embedding = get_embedding(code_query, engine='text-embedding-ada-002')\n",
    "    df['similarities'] = df.code_embedding.apply(lambda x: cosine_similarity(x, embedding))\n",
    "\n",
    "    res = df.sort_values('similarities', ascending=False).head(n)\n",
    "\n",
    "    if pprint:\n",
    "        for r in res.iterrows():\n",
    "            print(f\"{r[1].filepath}:{r[1].function_name}  score={round(r[1].similarities, 3)}\")\n",
    "            print(\"\\n\".join(r[1].code.split(\"\\n\")[:n_lines]))\n",
    "            print('-' * 70)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples\\low_level_api\\common.py:gpt_params_parse  score=0.727\n",
      "def gpt_params_parse(argv = None):\n",
      "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
      "    parser.add_argument(\"-s\", \"--seed\", type=int, default=-1, help=\"RNG seed (use random seed for <= 0)\",dest=\"seed\")\n",
      "    parser.add_argument(\"-t\", \"--threads\", type=int, default=min(4, os.cpu_count() or 1), help=\"number of threads to use during computation\",dest=\"n_threads\")\n",
      "    parser.add_argument(\"-n\", \"--n_predict\", type=int, default=128, help=\"number of tokens to predict (-1 = infinity)\",dest=\"n_predict\")\n",
      "    parser.add_argument(\"--n_parts\", type=int, default=-1, help=\"number of model parts\", dest=\"n_parts\")\n",
      "    parser.add_argument(\"-c\", \"--ctx_size\", type=int, default=512, help=\"size of the prompt context\",dest=\"n_ctx\")\n",
      "----------------------------------------------------------------------\n",
      "llama_cpp\\server\\app.py:make_logit_bias_processor  score=0.715\n",
      "def make_logit_bias_processor(\n",
      "    llama: llama_cpp.Llama,\n",
      "    logit_bias: Dict[str, float],\n",
      "    logit_bias_type: Optional[Literal[\"input_ids\", \"tokens\"]],\n",
      "):\n",
      "    if logit_bias_type is None:\n",
      "        logit_bias_type = \"input_ids\"\n",
      "----------------------------------------------------------------------\n",
      "tests\\test_llama.py:test_llama_patch  score=0.715\n",
      "def test_llama_patch(monkeypatch):\n",
      "    llama = llama_cpp.Llama(model_path=MODEL, vocab_only=True)\n",
      "    n_vocab = llama_cpp.llama_n_vocab(llama.ctx)\n",
      "\n",
      "    ## Set up mock function\n",
      "    def mock_eval(*args, **kwargs):\n",
      "        return 0\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "res = search_functions(df, 'fine-tuning input data validation logic', n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama_cpp\\llama_cpp.py:llama_token_suffix  score=0.74\n",
      "def llama_token_suffix(ctx: llama_context_p) -> int:\n",
      "    return _lib.llama_token_suffix(ctx)\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "vendor\\llama.cpp\\examples\\finetune\\convert-finetune-checkpoint-to-gguf.py:tensor_name  score=0.727\n",
      "def tensor_name(key, bid=None, suffix=\".weight\"):\n",
      "    return gguf.TENSOR_NAMES[key].format(bid=bid) + suffix\n",
      "\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "res = search_functions(df, 'find common suffix', n=2, n_lines=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples\\low_level_api\\common.py:gpt_params_parse  score=0.775\n",
      "def gpt_params_parse(argv = None):\n",
      "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
      "    parser.add_argument(\"-s\", \"--seed\", type=int, default=-1, help=\"RNG seed (use random seed for <= 0)\",dest=\"seed\")\n",
      "    parser.add_argument(\"-t\", \"--threads\", type=int, default=min(4, os.cpu_count() or 1), help=\"number of threads to use during computation\",dest=\"n_threads\")\n",
      "    parser.add_argument(\"-n\", \"--n_predict\", type=int, default=128, help=\"number of tokens to predict (-1 = infinity)\",dest=\"n_predict\")\n",
      "    parser.add_argument(\"--n_parts\", type=int, default=-1, help=\"number of model parts\", dest=\"n_parts\")\n",
      "    parser.add_argument(\"-c\", \"--ctx_size\", type=int, default=512, help=\"size of the prompt context\",dest=\"n_ctx\")\n",
      "    parser.add_argument(\"-b\", \"--batch_size\", type=int, default=8, help=\"batch size for prompt processing\",dest=\"n_batch\")\n",
      "    parser.add_argument(\"--keep\", type=int, default=0, help=\"number of tokens to keep from the initial prompt\",dest=\"n_keep\")\n",
      "\n",
      "    parser.add_argument(\n",
      "        \"-l\",\n",
      "        \"--logit-bias\",\n",
      "        type=str,\n",
      "        action='append',\n",
      "        help=\"--logit-bias TOKEN_ID(+/-)BIAS\",\n",
      "        dest=\"logit_bias_str\"\n",
      "    )\n",
      "    parser.add_argument(\"--ignore-eos\", action=\"store_true\", help=\"ignore end of stream token and continue generating\", dest=\"ignore_eos\")\n",
      "    parser.add_argument(\"--top_k\", type=int, default=40, help=\"top-k sampling\",dest=\"top_k\")\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "res = search_functions(df, 'Command line interface for fine-tuning', n=1, n_lines=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
