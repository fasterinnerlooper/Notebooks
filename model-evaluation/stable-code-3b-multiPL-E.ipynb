{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.13.1+cu116 requires torch==1.12.1, but you have torch 2.1.2 which is incompatible.\n",
      "thinc 8.1.7 requires pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4, but you have pydantic 2.5.3 which is incompatible.\n",
      "spacy 3.4.1 requires pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4, but you have pydantic 2.5.3 which is incompatible.\n",
      "spacy 3.4.1 requires typer<0.5.0,>=0.3.0, but you have typer 0.9.0 which is incompatible.\n",
      "scipy 1.9.2 requires numpy<1.26.0,>=1.18.5, but you have numpy 1.26.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU datasets transformers torch aiohttp numpy tqdm pytest datasets torch transformers evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
      "Hit:2 https://deb.nodesource.com/node_16.x focal InRelease                     \u001b[0m\n",
      "Get:3 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]      \u001b[0m\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu focal InRelease                         \u001b[0m\u001b[33m\u001b[33m\n",
      "Get:5 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Hit:6 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
      "Hit:7 http://archive.ubuntu.com/ubuntu focal-backports InRelease33m\n",
      "Get:8 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [32.5 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3770 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [3294 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1461 kB]\n",
      "Fetched 8786 kB in 2s (5090 kB/s)                          \u001b[0m\u001b[33m\u001b[33m\u001b[33m\u001b[33m\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "169 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "mono-devel is already the newest version (6.8.0.105+dfsg-2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 169 not upgraded.\n",
      "ln: failed to create symbolic link '/usr/bin/csc': File exists\n"
     ]
    }
   ],
   "source": [
    "!apt update\n",
    "!apt install -y mono-devel\n",
    "!ln -s /usr/bin/mono-csc /usr/bin/csc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mono C# compiler version 6.8.0.105\n"
     ]
    }
   ],
   "source": [
    "!csc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.26.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG = \"cs\"\n",
    "MODEL_NAME = \"stabilityai/stable-code-3b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d282fe554f9487a813b2aeb71b12748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StableLMEpochForCausalLM(\n",
       "  (model): StableLMEpochModel(\n",
       "    (embed_tokens): Embedding(50304, 2560)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x DecoderLayer(\n",
       "        (self_attn): Attention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "          (o_proj): Linear(in_features=2560, out_features=2560, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=6912, bias=False)\n",
       "          (up_proj): Linear(in_features=2560, out_features=6912, bias=False)\n",
       "          (down_proj): Linear(in_features=6912, out_features=2560, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, trust_remote_code=True, torch_dtype=\"auto\")\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = datasets.load_dataset(\"nuprl/MultiPL-E\", f\"humaneval-{LANG}\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run from here down to try a different Multi PL-E problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HumanEval_23_strlen(0): ✔️\n",
      "HumanEval_89_encrypt(1): ✔️\n",
      "HumanEval_95_check_dict_case(2): ✔️\n",
      "HumanEval_85_add(3): ✔️\n",
      "HumanEval_140_fix_spaces(4): ✔️\n",
      "HumanEval_63_fibfib(5): ✔️\n",
      "HumanEval_151_double_the_difference(6): ❌\n",
      "HumanEval_22_filter_integers(7): ✔️\n",
      "HumanEval_41_car_race_collision(8): ✔️\n",
      "HumanEval_17_parse_music(9): ✔️\n",
      "HumanEval_79_decimal_to_binary(10): ❌\n",
      "HumanEval_14_all_prefixes(11): ❌\n",
      "HumanEval_53_add(12): ✔️\n",
      "HumanEval_159_eat(13): ✔️\n",
      "HumanEval_115_max_fill(14): ❌\n",
      "HumanEval_160_do_algebra(15): ✔️\n",
      "HumanEval_27_flip_case(16): ❌\n",
      "HumanEval_105_by_length(17): ✔️\n",
      "HumanEval_25_factorize(18): ✔️\n",
      "HumanEval_96_count_up_to(19): ❌\n",
      "HumanEval_34_unique(20): ✔️\n",
      "HumanEval_74_total_match(21): ✔️\n",
      "HumanEval_35_max_element(22): ✔️\n",
      "HumanEval_132_is_nested(23): ❌\n",
      "HumanEval_113_odd_count(24): ❌\n",
      "HumanEval_109_move_one_ball(25): ✔️\n",
      "HumanEval_107_even_odd_palindrome(26): ❌\n",
      "HumanEval_138_is_equal_to_sum_even(27): ✔️\n",
      "HumanEval_62_derivative(28): ✔️\n",
      "HumanEval_126_is_sorted(29): ✔️\n",
      "HumanEval_161_solve(30): ✔️\n",
      "HumanEval_130_tri(31): ❌\n",
      "HumanEval_36_fizz_buzz(32): ✔️\n",
      "HumanEval_29_filter_by_prefix(33): ✔️\n",
      "HumanEval_84_solve(34): ✔️\n",
      "HumanEval_129_minPath(35): ❌\n",
      "HumanEval_98_count_upper(36): ✔️\n",
      "HumanEval_120_maximum(37): ❌\n",
      "HumanEval_24_largest_divisor(38): ✔️\n",
      "HumanEval_88_sort_array(39): ❌\n",
      "HumanEval_106_f(40): ✔️\n",
      "HumanEval_77_iscube(41): ✔️\n",
      "HumanEval_93_encode(42): ❌\n",
      "HumanEval_91_is_bored(43): ✔️\n",
      "HumanEval_43_pairs_sum_to_zero(44): ✔️\n",
      "HumanEval_71_triangle_area(45): ✔️\n",
      "HumanEval_148_bf(46): ✔️\n",
      "HumanEval_131_digits(47): ✔️\n",
      "HumanEval_101_words_string(48): ✔️\n",
      "HumanEval_18_how_many_times(49): ✔️\n",
      "HumanEval_51_remove_vowels(50): ❌\n",
      "HumanEval_70_strange_sort_list(51): ✔️\n",
      "HumanEval_20_find_closest_elements(52): ✔️\n",
      "HumanEval_76_is_simple_power(53): ✔️\n",
      "HumanEval_39_prime_fib(54): ✔️\n",
      "HumanEval_145_order_by_points(55): ❌\n",
      "HumanEval_0_has_close_elements(56): ✔️\n",
      "HumanEval_10_make_palindrome(57): ❌\n",
      "HumanEval_11_string_xor(58): ✔️\n",
      "HumanEval_139_special_factorial(59): ✔️\n",
      "HumanEval_122_add_elements(60): ✔️\n",
      "HumanEval_46_fib4(61): ✔️\n",
      "HumanEval_104_unique_digits(62): ❌\n",
      "HumanEval_117_select_words(63): ✔️\n",
      "HumanEval_72_will_it_fly(64): ❌\n",
      "HumanEval_55_fib(65): ✔️\n",
      "HumanEval_153_Strongest_Extension(66): ❌\n",
      "HumanEval_119_match_parens(67): ✔️\n",
      "HumanEval_90_next_smallest(68): ✔️\n",
      "HumanEval_92_any_int(69): ✔️\n",
      "HumanEval_2_truncate_number(70): ✔️\n",
      "HumanEval_42_incr_list(71): ✔️\n",
      "HumanEval_150_x_or_y(72): ✔️\n",
      "HumanEval_49_modp(73): ❌\n",
      "HumanEval_155_even_odd_count(74): ✔️\n",
      "HumanEval_80_is_happy(75): ✔️\n",
      "HumanEval_59_largest_prime_factor(76): ✔️\n",
      "HumanEval_66_digitSum(77): ✔️\n",
      "HumanEval_21_rescale_to_unit(78): ❌\n",
      "HumanEval_121_solution(79): ✔️\n",
      "HumanEval_68_pluck(80): ✔️\n",
      "HumanEval_147_get_max_triples(81): ❌\n",
      "HumanEval_110_exchange(82): ✔️\n",
      "HumanEval_47_median(83): ✔️\n",
      "HumanEval_82_prime_length(84): ❌\n",
      "HumanEval_73_smallest_change(85): ❌\n",
      "HumanEval_133_sum_squares(86): ✔️\n",
      "HumanEval_141_file_name_check(87): ❌\n",
      "HumanEval_40_triples_sum_to_zero(88): ✔️\n",
      "HumanEval_127_intersection(89): ✔️\n",
      "HumanEval_1_separate_paren_groups(90): ✔️\n",
      "HumanEval_152_compare(91): ❌\n",
      "HumanEval_83_starts_one_ends(92): ✔️\n",
      "HumanEval_134_check_if_last_char_is_a_letter(93): ✔️\n",
      "HumanEval_124_valid_date(94): ❌\n",
      "HumanEval_108_count_nums(95): ✔️\n",
      "HumanEval_86_anti_shuffle(96): ✔️\n",
      "HumanEval_48_is_palindrome(97): ✔️\n",
      "HumanEval_118_get_closest_vowel(98): ❌\n",
      "HumanEval_31_is_prime(99): ✔️\n",
      "HumanEval_144_simplify(100): ❌\n",
      "HumanEval_78_hex_key(101): ✔️\n",
      "HumanEval_143_words_in_sentence(102): ❌\n",
      "HumanEval_111_histogram(103): ✔️\n",
      "HumanEval_87_get_row(104): ✔️\n",
      "HumanEval_123_get_odd_collatz(105): ✔️\n",
      "HumanEval_135_can_arrange(106): ✔️\n",
      "HumanEval_19_sort_numbers(107): ✔️\n",
      "HumanEval_65_circular_shift(108): ❌\n",
      "HumanEval_142_sum_squares(109): ✔️\n",
      "HumanEval_94_skjkasdkd(110): ❌\n",
      "HumanEval_8_sum_product(111): ✔️\n",
      "HumanEval_102_choose_num(112): ✔️\n",
      "HumanEval_136_largest_smallest_integers(113): ❌\n",
      "HumanEval_16_count_distinct_characters(114): ✔️\n",
      "HumanEval_100_make_a_pile(115): ✔️\n",
      "HumanEval_128_prod_signs(116): ✔️\n",
      "HumanEval_114_minSubArraySum(117): ✔️\n",
      "HumanEval_15_string_sequence(118): ❌\n",
      "HumanEval_154_cycpattern_check(119): ✔️\n",
      "HumanEval_57_monotonic(120): ✔️\n",
      "HumanEval_12_longest(121): ✔️\n",
      "HumanEval_52_below_threshold(122): ✔️\n",
      "HumanEval_75_is_multiply_prime(123): ✔️\n",
      "HumanEval_30_get_positive(124): ✔️\n",
      "HumanEval_33_sort_third(125): ❌\n",
      "HumanEval_6_parse_nested_parens(126): ✔️\n",
      "HumanEval_45_triangle_area(127): ✔️\n",
      "HumanEval_97_multiply(128): ✔️\n",
      "HumanEval_4_mean_absolute_deviation(129): ✔️\n",
      "HumanEval_58_common(130): ✔️\n",
      "HumanEval_156_int_to_mini_roman(131): ❌\n",
      "HumanEval_67_fruit_distribution(132): ✔️\n",
      "HumanEval_112_reverse_delete(133): ✔️\n",
      "HumanEval_13_greatest_common_divisor(134): ✔️\n",
      "HumanEval_116_sort_array(135): ❌\n",
      "HumanEval_28_concatenate(136): ✔️\n",
      "HumanEval_149_sorted_list_sum(137): ✔️\n",
      "HumanEval_7_filter_by_substring(138): ✔️\n",
      "HumanEval_99_closest_integer(139): ✔️\n",
      "HumanEval_64_vowels_count(140): ✔️\n",
      "HumanEval_158_find_max(141): ✔️\n",
      "HumanEval_162_string_to_md5(142): ✔️\n",
      "HumanEval_44_change_base(143): ✔️\n",
      "HumanEval_157_right_angle_triangle(144): ✔️\n",
      "HumanEval_81_numerical_letter_grade(145): ❌\n",
      "HumanEval_5_intersperse(146): ✔️\n",
      "HumanEval_146_specialFilter(147): ✔️\n",
      "HumanEval_60_sum_to_n(148): ✔️\n",
      "HumanEval_26_remove_duplicates(149): ✔️\n",
      "HumanEval_163_generate_integers(150): ✔️\n",
      "HumanEval_9_rolling_max(151): ✔️\n",
      "HumanEval_3_below_zero(152): ✔️\n",
      "HumanEval_69_search(153): ✔️\n",
      "HumanEval_61_correct_bracketing(154): ✔️\n",
      "HumanEval_37_sort_even(155): ✔️\n",
      "HumanEval_54_same_chars(156): ✔️\n",
      "HumanEval_56_correct_bracketing(157): ✔️\n"
     ]
    }
   ],
   "source": [
    "problem_len = len(problems['test'])\n",
    "mid_tok = tokenizer(\"<fim_middle>\")['input_ids'][0]\n",
    "references = []\n",
    "predictions = []\n",
    "\n",
    "for x in range(problem_len):\n",
    "  problem = problems['test'][x]\n",
    "  prompt = problem['prompt']\n",
    "  tests = problem['tests']\n",
    "  fim = f\"<fim_prefix>{prompt}<fim_suffix>{tests}<fim_middle>\"\n",
    "  inputs = tokenizer(fim, return_tensors=\"pt\").to(model.device)\n",
    "  tokens = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.2,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    "  )\n",
    "  mid_pos = (tokens[0]==mid_tok).nonzero().item()\n",
    "  masked_index = torch.nonzero(tokens[0] == mid_tok, as_tuple=False)[0].item()\n",
    "  fim = tokenizer.decode(tokens[0][masked_index:], skip_special_tokens=True)\n",
    "  with open(\"program.cs\", \"w\", encoding='utf8') as doc:\n",
    "    doc.write(prompt)\n",
    "    doc.write(fim)\n",
    "    doc.write(tests)\n",
    "  import subprocess\n",
    "  build = subprocess.run(['csc','/d:DEBUG','-r:System.Numerics.dll', 'program.cs', '/out:program.exe'], capture_output=True)\n",
    "  references.append(1 if build.returncode == 0 else 0)\n",
    "  predictions.append(1)\n",
    "  print(problem['name']+f'({x}): {\"✔️\" if build.returncode == 0 else \"❌\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.7531645569620253}\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "results = accuracy.compute(references=references, predictions=predictions)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
