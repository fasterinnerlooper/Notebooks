{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU transformers bitsandbytes optimum accelerate torch python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "%reload_ext dotenv\n",
    "%dotenv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login, login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.36.2\n",
      "Accelerate version: 0.26.1\n",
      "PyTorch version: 2.1.2+cu121\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import accelerate\n",
    "import torch\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Accelerate version: {accelerate.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('model.embed_tokens', 0),\n",
       "             ('model.layers.0', 0),\n",
       "             ('model.layers.1', 0),\n",
       "             ('model.layers.2', 0),\n",
       "             ('model.layers.3', 0),\n",
       "             ('model.layers.4', 0),\n",
       "             ('model.layers.5', 0),\n",
       "             ('model.layers.6', 0),\n",
       "             ('model.layers.7', 0),\n",
       "             ('model.layers.8', 0),\n",
       "             ('model.layers.9', 0),\n",
       "             ('model.layers.10', 0),\n",
       "             ('model.layers.11', 0),\n",
       "             ('model.layers.12', 0),\n",
       "             ('model.layers.13', 0),\n",
       "             ('model.layers.14', 0),\n",
       "             ('model.layers.15', 0),\n",
       "             ('model.layers.16', 0),\n",
       "             ('model.layers.17', 0),\n",
       "             ('model.layers.18', 'cpu'),\n",
       "             ('model.layers.19', 'cpu'),\n",
       "             ('model.layers.20', 'cpu'),\n",
       "             ('model.layers.21', 'cpu'),\n",
       "             ('model.layers.22', 'cpu'),\n",
       "             ('model.layers.23', 'cpu'),\n",
       "             ('model.layers.24', 'cpu'),\n",
       "             ('model.layers.25', 'cpu'),\n",
       "             ('model.layers.26', 'cpu'),\n",
       "             ('model.layers.27', 'cpu'),\n",
       "             ('model.layers.28', 'cpu'),\n",
       "             ('model.layers.29', 'cpu'),\n",
       "             ('model.layers.30', 'cpu'),\n",
       "             ('model.layers.31', 'cpu'),\n",
       "             ('model.norm', 'cpu'),\n",
       "             ('lm_head', 'cpu')])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from accelerate import infer_auto_device_map, init_empty_weights\n",
    "from transformers import (\n",
    "  AutoConfig,\n",
    "  AutoModelForCausalLM,\n",
    "  AutoTokenizer,\n",
    "  TextStreamer\n",
    ")\n",
    "\n",
    "checkpoint = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "with init_empty_weights():\n",
    "  model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "device_map = infer_auto_device_map(model, no_split_module_classes=[\"LlamaDecoderLayer\"])\n",
    "device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd7a68112f9a4a4fbe37a074779ffd33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "device_map[\"lm_head\"] = 0\n",
    "device_map[\"model.decoder.layers.17\"] = \"cpu\"\n",
    "device_map[\"model.decoder.layers.16\"] = \"cpu\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint,\n",
    "    device_map=device_map,\n",
    "    offload_folder=\"offload\",\n",
    "    offload_state_dict = True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a script written in Python to find the RMS for a given sequence of "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numbers:\n",
      "\n",
      "\\begin{code}\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def rms(x):\n",
      "   return np.sqrt(np.mean(np.square(x)))\n",
      "\n",
      "x = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
      "\n",
      "plt.plot(x, label='Original')\n",
      "plt.plot(rms(x), label='RMS')\n",
      "plt.xlabel('Values')\n",
      "plt.ylabel('RMS')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\\end{code}\n",
      "\n",
      "<a href=\"https://i.stack.imgur.com/6Fv7J.png\" rel=\"nofollow noreferrer\"><IMAGE></a>\n",
      "\n",
      "The function rms is defined as:\n",
      "\n",
      "\\begin{code}\n",
      "def rms(x\n"
     ]
    }
   ],
   "source": [
    "streamer = TextStreamer(tokenizer, skip_special_tokens=True)\n",
    "\n",
    "# input_text = \"Hello my dog is cute and\"\n",
    "# input_text=\"When you look at the USA through a European's eyes, there is something to\"\n",
    "input_text=\"Here is a script written in Python to find the RMS for a given sequence of numbers:\"\n",
    "# input_text=\"The difference between Harry Potter and Hunger Games is\"\n",
    "# input_text=\"Piglet, Pooh's best friend, likes to\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# enable FlashAttention\n",
    "with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n",
    "  _ = model.generate(**inputs, max_new_tokens=200, temperature=0.7, streamer=streamer, do_sample=True)\n",
    "\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
