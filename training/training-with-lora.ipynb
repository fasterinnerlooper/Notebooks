{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0450998a",
   "metadata": {},
   "source": [
    "# Training with LoRA\n",
    "\n",
    "We'll be training a selected model with LoRA against the LCC_CSharp dataset that has been adjusted for the task of in-filling or fill-in-the-middle (FIM)\n",
    "\n",
    "We'll then be evaluating the model on the Multi PL-E benchmark, which is a multiple language representation of the HumanEval benchmark. We'll solely be focusing on the C# code within the benchmark as our goal is to create a competent C# Generative LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8730683d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=False\n"
     ]
    }
   ],
   "source": [
    "%env TOKENIZERS_PARALLELISM=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3ce175c-53f9-4552-b0b9-b9573fc4e811",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-14T14:45:41.224457Z",
     "iopub.status.busy": "2024-01-14T14:45:41.224015Z",
     "iopub.status.idle": "2024-01-14T14:47:49.933948Z",
     "shell.execute_reply": "2024-01-14T14:47:49.933165Z",
     "shell.execute_reply.started": "2024-01-14T14:45:41.224429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers==4.36.2 accelerate==0.26.1 evaluate datasets peft==0.7.1 bitsandbytes trl python-dotenv wandb -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c00dbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
      "Hit:2 http://security.ubuntu.com/ubuntu focal-security InRelease               \u001b[0m\n",
      "Hit:3 https://deb.nodesource.com/node_16.x focal InRelease          \u001b[0m       \n",
      "Hit:4 http://archive.ubuntu.com/ubuntu focal InRelease              \n",
      "Hit:5 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Hit:7 http://archive.ubuntu.com/ubuntu focal-backports InRelease\n",
      "Fetched 114 kB in 1s (145 kB/s)\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "169 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "mono-devel is already the newest version (6.8.0.105+dfsg-2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 169 not upgraded.\n",
      "ln: failed to create symbolic link '/usr/bin/csc': File exists\n"
     ]
    }
   ],
   "source": [
    "!apt update\n",
    "!apt install -y mono-devel\n",
    "!ln -s /usr/bin/mono-csc /usr/bin/csc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ca8dcc2-3549-421f-993f-6241e13a63f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.36.2\n",
      "Accelerate version: 0.26.1\n",
      "PEFT version: 0.7.1\n",
      "PyTorch version: 2.1.2+cu121\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import accelerate\n",
    "import peft\n",
    "import torch\n",
    "\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Accelerate version: {accelerate.__version__}\")\n",
    "print(f\"PEFT version: {peft.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6845b1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext dotenv\n",
    "%dotenv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "173f0714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"] = \"csharp-stable-code\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c75c4a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig\n",
    "from peft import LoraConfig\n",
    "import evaluate\n",
    "import re\n",
    "from transformers import TrainingArguments\n",
    "from datetime import datetime\n",
    "from random import randint\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "model_id=\"stabilityai/stable-code-3b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28ee3e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79fe7efd7c3b40cb84d6314292f9817e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "  load_in_4bit=True,\n",
    "  bnb_4bit_compute_dtype=torch.float16,\n",
    "  bnb_4bit_quant_type=\"nf4\",\n",
    "  # llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_id,\n",
    "  trust_remote_code=True,\n",
    "  # device_map=device_map,\n",
    "  # offload_folder=\"offload\",\n",
    "  # offload_state_dict = True,\n",
    "  # torch_dtype=torch.float16,\n",
    "  quantization_config=quantization_config\n",
    "  )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "  r=8,\n",
    "  target_modules=[\n",
    "    \"q_proj\",\n",
    "    \"o_proj\",\n",
    "    \"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"up_proj\",\n",
    "    \"down_proj\",\n",
    "  ],\n",
    "  bias=\"none\",\n",
    "  task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model.add_adapter(lora_config)\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\":False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b678ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.add_tokens(['<fim_prefix>','<fim_suffix>','<fim_middle>'])\n",
    "# model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a519fec3-3253-419f-8973-0c9deba126b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "shards=100\n",
    "# from datasets import load_from_disk\n",
    "# train_dataset=load_from_disk('./train_dataset/')\n",
    "# eval_dataset=load_from_disk('./eval_dataset/')\n",
    "raw_dataset=load_dataset(\"fasterinnerlooper/lcc_csharp\")['train']\n",
    "raw_dataset=raw_dataset.train_test_split()\n",
    "train_dataset=raw_dataset['train'].shard(shards, randint(1, shards-1))\n",
    "eval_dataset=raw_dataset['test'].shard(shards,randint(1, shards-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f8c1fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(source):\n",
    "  # Written for stable-code-3b\n",
    "  ret = \"<fim_prefix>\"+source['prefix']+\"<fim_suffix>\"+source['suffix']+\"<fim_middle>\"\n",
    "  return {'input':ret}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a325386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "LANG = \"cs\"\n",
    "\n",
    "def compute_metrics():\n",
    "  problems = datasets.load_dataset(\"nuprl/MultiPL-E\", f\"humaneval-{LANG}\", trust_remote_code=True)\n",
    "\n",
    "  problem_len = len(problems['test'])\n",
    "  mid_tok = tokenizer(\"<fim_middle>\")['input_ids'][0]\n",
    "  references = []\n",
    "  predictions = []\n",
    "\n",
    "  for x in range(problem_len):\n",
    "    problem = problems['test'][x]\n",
    "    prompt = problem['prompt']\n",
    "    tests = problem['tests']\n",
    "    fim = f\"<fim_prefix>{prompt}<fim_suffix>{tests}<fim_middle>\"\n",
    "    inputs = tokenizer(fim, return_tensors=\"pt\").to(model.device)\n",
    "    tokens = model.generate(\n",
    "      **inputs,\n",
    "      max_new_tokens=200,\n",
    "      temperature=0.2,\n",
    "      do_sample=True,\n",
    "      pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    mid_pos = (tokens[0]==mid_tok).nonzero().item()\n",
    "    masked_index = torch.nonzero(tokens[0] == mid_tok, as_tuple=False)[0].item()\n",
    "    fim = tokenizer.decode(tokens[0][masked_index:], skip_special_tokens=True)\n",
    "    with open(\"program.cs\", \"w\", encoding='utf8') as doc:\n",
    "      doc.write(prompt)\n",
    "      doc.write(fim)\n",
    "      doc.write(tests)\n",
    "    import subprocess\n",
    "    build = subprocess.run(['csc','/d:DEBUG','-r:System.Numerics.dll', 'program.cs', '/out:program.exe'], capture_output=True)\n",
    "    references.append(1 if build.returncode == 0 else 0)\n",
    "    predictions.append(1)\n",
    "    print(problem['name']+f'({x}): {\"✔️\" if build.returncode == 0 else \"❌\"}')\n",
    "  accuracy = evaluate.load(\"accuracy\")\n",
    "  results = accuracy.compute(references=references, predictions=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a76ab667",
   "metadata": {},
   "outputs": [],
   "source": [
    "YOUR_HF_USERNAME = \"fasterinnerlooper\"\n",
    "\n",
    "output_dir = re.sub(r'.*/',f'{YOUR_HF_USERNAME}/', model_id)\n",
    "per_device_train_batch_size = 2\n",
    "gradient_accumulation_steps = 4\n",
    "optim = \"paged_adamw_8bit\"\n",
    "save_steps = 100\n",
    "logging_steps = 10\n",
    "learning_rate = 3e-5\n",
    "max_grad_norm = 0.3\n",
    "max_steps = 50\n",
    "warmup_ratio = 0.3\n",
    "lr_scheduler_type = \"cosine\"\n",
    "num_epochs=20\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"stable-code-training-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\",\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    # learning_rate=learning_rate,\n",
    "    # max_grad_norm=max_grad_norm,\n",
    "    # max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    gradient_checkpointing=True,\n",
    "    push_to_hub=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    # resume_from_checkpoint=f'{output_dir}/checkpoint-130',\n",
    "    save_total_limit=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_accumulation_steps=1,\n",
    "    load_best_model_at_end=True,\n",
    "    # num_train_epochs=num_epochs\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18b641c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e737cdd43924f83a5535b8180d98306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8aee9073704155b931d228a7575b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    packing=False,\n",
    "    dataset_text_field=\"prediction\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=512,\n",
    "    formatting_func=formatting_func,\n",
    "    compute_metrics=compute_metrics,\n",
    "    peft_config=lora_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10603afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshafiq-jetha\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/wandb/run-20240124_154418-byiua6tb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shafiq-jetha/csharp-stable-code/runs/byiua6tb' target=\"_blank\">stable-code-training-2024-01-24-15-44</a></strong> to <a href='https://wandb.ai/shafiq-jetha/csharp-stable-code' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shafiq-jetha/csharp-stable-code' target=\"_blank\">https://wandb.ai/shafiq-jetha/csharp-stable-code</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shafiq-jetha/csharp-stable-code/runs/byiua6tb' target=\"_blank\">https://wandb.ai/shafiq-jetha/csharp-stable-code/runs/byiua6tb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/usr/local/lib/python3.9/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='279' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 11/279 01:35 < 47:24, 0.09 it/s, Epoch 0.11/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='89' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 89/125 06:43 < 02:44, 0.22 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bce74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ed8a7b323f44dbb3c907d2147f50f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='49.846 MB of 49.846 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▂▁</td></tr><tr><td>eval/runtime</td><td>█▃▁</td></tr><tr><td>eval/samples_per_second</td><td>▁▅█</td></tr><tr><td>eval/steps_per_second</td><td>▁▅█</td></tr><tr><td>train/epoch</td><td>▁▅██</td></tr><tr><td>train/global_step</td><td>▁▅██</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.20266</td></tr><tr><td>eval/runtime</td><td>17.5745</td></tr><tr><td>eval/samples_per_second</td><td>1.423</td></tr><tr><td>eval/steps_per_second</td><td>1.423</td></tr><tr><td>train/epoch</td><td>2.67</td></tr><tr><td>train/global_step</td><td>50</td></tr><tr><td>train/total_flos</td><td>2194587209428992.0</td></tr><tr><td>train/train_loss</td><td>0.97225</td></tr><tr><td>train/train_runtime</td><td>527.0506</td></tr><tr><td>train/train_samples_per_second</td><td>0.379</td></tr><tr><td>train/train_steps_per_second</td><td>0.095</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stable-code-training-2024-01-22-01-01</strong> at: <a href='https://wandb.ai/shafiq-jetha/csharp-stable-code/runs/873bbnhp' target=\"_blank\">https://wandb.ai/shafiq-jetha/csharp-stable-code/runs/873bbnhp</a><br/>Synced 5 W&B file(s), 0 media file(s), 7 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240122_010204-873bbnhp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
